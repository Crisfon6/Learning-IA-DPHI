{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Performance_Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdPA03q3vRei",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JuiXnrL99eZzIiRzdtREyLlCYhF6Nc4u?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4yt1psqgh1X",
        "colab_type": "text"
      },
      "source": [
        "# Performance Evaluation, Cross Validation and Hyper - Parameter Tunning\n",
        "In this Notebook, we will learn 3 things:   \n",
        "*  Evaluation metrics\n",
        "*  Cross Validation\n",
        "*  Hyperparameter Tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57GJ0KnnpC34",
        "colab_type": "text"
      },
      "source": [
        "### Data Description\n",
        "Here we will use diabetes dataset for classification. Given different medical specifications about a person, we have to predict if the person have diabetes or not.\n",
        "\n",
        "**Different Attributes:**\n",
        "1. Number of times pregnant\n",
        "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "3. Diastolic blood pressure (mm Hg)\n",
        "4. Triceps skin fold thickness (mm)\n",
        "5. 2-Hour serum insulin (mu U/ml)\n",
        "6. Body mass index (weight in kg/(height in m)^2)\n",
        "7. Diabetes pedigree function\n",
        "8. Age (years)\n",
        "9. Class variable (0 or 1)\n",
        "\n",
        "All the feature names are numerical. Let's give textual names to these features.\n",
        "*  Number of times pregnant: **num_preg**\n",
        "*  Plasma glucose concentration a 2 hours in an oral glucose tolerance test: **plasma_glucose_conc**\n",
        "*  Diastolic blood pressure (mm Hg): **D_blood_pressure**\n",
        "*  Triceps skin fold thickness (mm): **skin_fold_thickness**\n",
        "*  2-Hour serum insulin (mu U/ml): **serum_insulin**\n",
        "*  Body mass index (weight in kg/(height in m)^2): **body_mass_index**\n",
        "*  Diabetes pedigree function: **pedigree_func**\n",
        "*  Age (years): **age**\n",
        "*  Class variable (0 or 1): **diabetes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYnubTr4oUKY",
        "colab_type": "text"
      },
      "source": [
        "### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WQljMFOgh1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRSnYQG9okA4",
        "colab_type": "text"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUaCOsuroooy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# since the column names are numerical, we will give our own column names for our understanding\n",
        "col = [\"num_preg\", \"plasma_glucose_conc\", \"D_blood_pressure\", \"skin_fold_thickness\", \"serum_insulin\", \"body_mass_index\", \"pedigree_func\", \"age\", \"diabetes\"]\n",
        "diabetes_data = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/diabetes.txt\", names = col)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3Gg7tKkv8uw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "774d62c7-e6b5-45be-9389-6f08589873b4"
      },
      "source": [
        "diabetes_data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_preg</th>\n",
              "      <th>plasma_glucose_conc</th>\n",
              "      <th>D_blood_pressure</th>\n",
              "      <th>skin_fold_thickness</th>\n",
              "      <th>serum_insulin</th>\n",
              "      <th>body_mass_index</th>\n",
              "      <th>pedigree_func</th>\n",
              "      <th>age</th>\n",
              "      <th>diabetes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num_preg  plasma_glucose_conc  ...  age  diabetes\n",
              "0         6                  148  ...   50         1\n",
              "1         1                   85  ...   31         0\n",
              "2         8                  183  ...   32         1\n",
              "3         1                   89  ...   21         0\n",
              "4         0                  137  ...   33         1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbHhT_yMgh1p",
        "colab_type": "text"
      },
      "source": [
        "**This dataset contains 13 columns and based on different features, it is guessed whether or not a person has Diabetes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Eap_ejyyJ5",
        "colab_type": "text"
      },
      "source": [
        "### Separating Input variables and output variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly3PQ9DayxWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = diabetes_data.drop('diabetes', axis = 1)\n",
        "y = diabetes_data.diabetes"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOQVck62gh1z",
        "colab_type": "text"
      },
      "source": [
        "#### Split into training and testing (80:20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AylKRHMgh12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
        "\n",
        "# The below line of code will not need to separate input variables and output variables.\n",
        "# The code is very simple if you remember numpy and pandas session. Indexing dataframe and arrays\n",
        "# x_train, x_test, y_train, y_test = train_test_split(diabetes.iloc[:, :-1], diabetes.iloc[:,-1], test_size=0.2, random_state=3)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3R17-E7kqjD",
        "colab_type": "text"
      },
      "source": [
        "**Note for learners:** Here we have used MLPClassifier from neural_network module of sklearn library. MLP Classifier is also a classification algorithm like logistic regression or decision tree. We will soon learn about Neural Networks and Artificial Neural Networks in the upcoming sessions. So, no need to worry about it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZizeBIHwgh17",
        "colab_type": "text"
      },
      "source": [
        "### Developing a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqeygkPIgh18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(max_iter=1000)\n",
        "mlp.fit(x_train, y_train)\n",
        "y_pred = mlp.predict(x_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8-k13W0gh1T",
        "colab_type": "text"
      },
      "source": [
        "## Performance Evaluation\n",
        "Evaluating performance of the machine learning model that we have built is an essential part of any machine learning project. Performance of our model is done using some evaluation metrics. Accuracy score is one among them.\n",
        "\n",
        "**Why accuracy score is not a good evaluation metric?**\n",
        "\n",
        "Our model may give satisfying results if we use accuracy score for a particular dataset but at the same time accuracy score is not a good measure of evaluation for some particular dataset like fraud detection that we discussed during class imbalance problem. Let's consider the same dataset, suppose we have 1000 transaction in the dataset. Out of 1000 transactions 20 transactions are fraud transactions. Now let's say you build a model which predicted all the 1000 transactions as not fraud transaction, for 980 transaction which were not fraud, the prediction is correct while the transaction which were actually fraud are also predicted as not fraud. The accuracy is nothing but total correct prediction divided by total prediction. In this case we have total prediction as 1000 (as we have 1000 transactions) while total correct prediction is 980, resulting the accuracy score of 980/1000 = 0.98. The model is giving 98% of accuracy. Do you think the model is good? No, because our model could not notice the transaction which were actually fraud. \n",
        "\n",
        "Here we will discuss some other metrices for both classification and regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D9DMXXigh1Z",
        "colab_type": "text"
      },
      "source": [
        "## 1. Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD9O15axgh2G",
        "colab_type": "text"
      },
      "source": [
        "**All performance metrics in sklearn are to be written in the same way -**  \n",
        "> ``` metric_function(true_label, predicted_labels) ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V1Az3YsBUgP",
        "colab_type": "text"
      },
      "source": [
        "Below are the metrics for classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtZ2yI1Lgh2I",
        "colab_type": "text"
      },
      "source": [
        "### Confusion Matrix\n",
        "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.\n",
        "\n",
        "Further reading about confusion matrix and its related terminologies: \n",
        "1. https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
        "2. https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKr_nFuRgh2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3S3OpD6QLmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1d876879-b267-4161-96d1-ca5a45517f4f"
      },
      "source": [
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[84,  8],\n",
              "       [40, 22]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12hGE_5rgh2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1609c4d8-c4be-414a-e3eb-e0d1cd822a15"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()     # ravel() is used to convert a 2D array to 1D array. The output by confusion matrix is a 2D array.\n",
        "print(\"True Positive\", tp)\n",
        "print(\"True Negative\", tn)\n",
        "print(\"False Positive\", fp)\n",
        "print(\"False Negative\", fn)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positive 22\n",
            "True Negative 84\n",
            "False Positive 8\n",
            "False Negative 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqjqAVSQgh2S",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy\n",
        "\\begin{align}\n",
        "Accuracy = \\frac{TP+TN}{TP+TN+FN+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX3DELNUgh2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64ae370b-5bb6-4e7b-9003-aa054a559a15"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "acc"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6883116883116883"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNFBGQaCEjCA",
        "colab_type": "text"
      },
      "source": [
        "**When is it good to use accuracy score as a model evaluation metric?**\n",
        "1. The classifications in the dataset is nearly symmetrical (means equal distribution of all the classes).\n",
        "2. The false positive and false negative on test data are nearly equal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh4fSgV9gh2a",
        "colab_type": "text"
      },
      "source": [
        "### Recall (Sensitivity)\n",
        "\\begin{align}\n",
        "Sensitivity = \\frac{TP}{TP+FN}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGBdDw9hgh2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import recall_score"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk9ffog9gh2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb8659fe-beb3-40d2-e7cb-dffb4181d58e"
      },
      "source": [
        "recall_score(y_test, y_pred)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3548387096774194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlQf5BUtgh2o",
        "colab_type": "text"
      },
      "source": [
        "### Specificity\n",
        "sklearn does not have an inbuild function for Specificity. But by adding parameter pos_label =0 to the recall function, we treat that as the positive class, and hence gives the correct output\n",
        "\\begin{align}\n",
        "Specificity = \\frac{TN}{TN+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHTxK6NHgh2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8cfa035a-5e40-427e-93e9-ee0aec82a3b9"
      },
      "source": [
        "print(\"Specificity with recall pos label=0: \",recall_score(y_test, y_pred, pos_label=0))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Specificity with recall pos label=0:  0.9130434782608695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UAM0Kz2gh2u",
        "colab_type": "text"
      },
      "source": [
        "**Checking with formulas (tn , fp from confusion matrix):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrthV4dhgh2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b4cab2c-947b-44d5-c8bc-58801122d7f8"
      },
      "source": [
        "print(\"Specificity with Formulas: \", tn/(tn+fp))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Specificity with Formulas:  0.9130434782608695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7QrITQBgh21",
        "colab_type": "text"
      },
      "source": [
        "They are the same! You can use either one of them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plCwrQB_gh23",
        "colab_type": "text"
      },
      "source": [
        "### Precision\n",
        "\\begin{align}\n",
        "Precision = \\frac{TP}{TP+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14pynqe6gh25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01bb27de-fc3a-4900-cd09-3d1ade50c744"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "precision_score(y_test, y_pred)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_2Y9vQ9gh2_",
        "colab_type": "text"
      },
      "source": [
        "### Imbalanced Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwMwh4mhgh3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "aa285feb-109e-4a6e-8d71-b734a5908884"
      },
      "source": [
        "diabetes_data.iloc[:,-1].value_counts()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    500\n",
              "1    268\n",
              "Name: diabetes, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY7eJaxsgh3K",
        "colab_type": "text"
      },
      "source": [
        "### Matthews Correlation Coefficient\n",
        "\\begin{align}\n",
        "MCC = \\frac{(TP*TN)-(FP*FN)}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr62zEHWgh3L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cefc5e17-cf17-4b2c-d387-2175a9c230e8"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "print(\"MCC Score: \",matthews_corrcoef(y_test, y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC Score:  0.3317127203003536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hum7bVaAgh3P",
        "colab_type": "text"
      },
      "source": [
        "### F1 Score\n",
        "It is the harmonic mean of Precision and recall\n",
        "\n",
        "\\begin{align}\n",
        "Precision = \\frac{2*Precision*Recall}{Precision+Recall}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3A9uIKwgh3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd5c7de5-7982-474d-d193-e917065ea974"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 Score: \",f1_score(y_test, y_pred))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score:  0.47826086956521735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVcVsBEhgh3V",
        "colab_type": "text"
      },
      "source": [
        "## Area Under the Curve (Reciever Operating Characterstics)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YypfoTaVgh3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZPwqn0Dgh3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2a1fd06d-3bd1-46a4-c49b-95061c324949"
      },
      "source": [
        "plot_roc_curve(mlp, x_test, y_test)\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHrohQufgh3l",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Regression Evaluation Metrics "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUc_2Fz7gh3m",
        "colab_type": "text"
      },
      "source": [
        "Wine Dataset  \n",
        "    <b> Predictor Variable: </b> Quality (Tells quality of wine)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMZbaFOcgh3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "538db3cc-beaf-4aab-e5cb-6cf0b9cfb833"
      },
      "source": [
        "wine = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/winequality.csv\", sep=\";\")\n",
        "wine.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0010</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.9940</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0            7.0              0.27         0.36  ...       0.45      8.8        6\n",
              "1            6.3              0.30         0.34  ...       0.49      9.5        6\n",
              "2            8.1              0.28         0.40  ...       0.44     10.1        6\n",
              "3            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "4            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9dBEH_895Xx",
        "colab_type": "text"
      },
      "source": [
        "### Separate Input and Output Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIuUR4Hy9459",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = wine.drop('quality', axis = 1)\n",
        "y = wine.quality"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP8EUDXTgh3s",
        "colab_type": "text"
      },
      "source": [
        "#### Split into training and testing (80:20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGgMF3Rzgh3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we are performing both separation of input and output variable and the splitting.\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnuT2aaQgh3z",
        "colab_type": "text"
      },
      "source": [
        "Creating a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsRa3Pe0gh32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PMAIOYRgh4B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "48cee167-c47f-4660-fc62-743077bd4328"
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)\n",
        "y_pred[:10]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.44455619, 5.57868309, 5.99091469, 5.19864346, 6.0666099 ,\n",
              "       5.01639077, 5.68416174, 6.26611011, 5.97010538, 5.65519351])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEQuSCzMgh4R",
        "colab_type": "text"
      },
      "source": [
        "## Performance Measurement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FZ86Xpbgh4U",
        "colab_type": "text"
      },
      "source": [
        "Let y = Actual Value,  $\\tilde{y}$ = Predicted Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjIBhaItgh4X",
        "colab_type": "text"
      },
      "source": [
        "### Mean Absolute Error  \n",
        "*  MAE is the absolute difference between the target value and the value predicted by the model.  \n",
        "*  The MAE is more robust to outliers and does not penalize the errors as extremely as mse\n",
        "\\begin{align}\n",
        "MAE  = \\frac{1}{n}\\sum|y-\\tilde{y}| \n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNOb1yb2gh4Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dec0faf1-84bc-4c3e-df8a-04405e7adf12"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_pred)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5972358558776472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHrz_JZAgh4j",
        "colab_type": "text"
      },
      "source": [
        "### Mean Squared Error\n",
        "*  It is simply the average of the squared difference between the target value and the value predicted by the regression model. \n",
        "*  As it squares the differences, it penalizes even a small error which leads to over-estimation of how bad the model is.\n",
        "*  MSE or Mean Squared Error is one of the most preferred metrics for regression tasks. \n",
        "\\begin{align}\n",
        "MSE & = \\frac{1}{n}\\sum(y-\\tilde{y})^2\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHZcMiDrgh4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a3fc73f-a723-4623-89e7-560bc70aa012"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Mean Squared Error: \",mean_squared_error(y_test, y_pred))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error:  0.5906658099548077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx7Dtz5Wgh4r",
        "colab_type": "text"
      },
      "source": [
        "### Root Mean Square Error\n",
        "*  RMSE is the square root of the averaged squared difference between the target value and the value predicted by the model. \n",
        "*  It is preferred more in some cases because the errors are first squared before averaging which poses a high penalty on large errors.\n",
        "*  This implies that RMSE is useful when large errors are undesired.\n",
        "\\begin{align}\n",
        "RMSE  = \\sqrt{\\frac{1}{n}\\sum(y-\\tilde{y})^2}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "EfQI1JFzgh4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e603675-172b-4c2b-cb44-532676ae5254"
      },
      "source": [
        "print(\"Root Mean Squared Error: \",mean_squared_error(y_test, y_pred, squared=False))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error:  0.7685478579469256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onetrj_Agh5A",
        "colab_type": "text"
      },
      "source": [
        "### R Squared\n",
        "<li>The metric helps us to compare our current model with a constant baseline and tells us how much our model is better\n",
        "\\begin{align}\n",
        "R^2 = 1 - \\frac{MSE(Model)}{MSE(Baseline)}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmTpqG2rgh5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd5bc780-3f3c-48bc-f666-b5ddeee8f8ed"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, y_pred)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2832037191111023"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoVhiVKgh5H",
        "colab_type": "text"
      },
      "source": [
        "# 2. Cross Validation\n",
        "Usually, our data is divided into Train and Test Sets.\n",
        "The Train set is further divided into Train and Validation set. \n",
        "\n",
        "The Validation Set helps us in selecting good parameters/tune the parameters for our model.\n",
        "\n",
        "This Three fold set can be seen in the figure below:\n",
        "\n",
        "![train-test-val](https://amueller.github.io/ml-training-intro/slides/images/threefold_split.png)\n",
        "\n",
        "Our dataset should be as large as possible to train the model and removing considerable part of it for validation poses a problem of losing valuable portion of data that we would prefer to be able to train. \n",
        "\n",
        "In order to address this issue, we use the Cross validation technique. Cross Validation has a number of types out of which we'll be using K-fold cross validation today.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dGdSReX-iFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here the diabetes data is use\n",
        "# separate input and output vairables\n",
        "X = diabetes_data.drop('diabetes', axis = 1)\n",
        "y = diabetes_data.diabetes"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gESg0k8_gh5H",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 K-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKX8Gjyugh5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_validate"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7kl2Ah_gh5Q",
        "colab_type": "text"
      },
      "source": [
        "<blockquote> We can also import cross_val_score from the same library, but it only allows a single scorer to be implemented. So we are using cross_validate </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GiU8TBWgh5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "82db5f2a-9b08-49cd-d1d4-d2b19d970970"
      },
      "source": [
        "cv_results = cross_validate(mlp, X, y, cv=10, scoring=[\"accuracy\", \"precision\", \"recall\"])\n",
        "cv_results"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': array([0.62892437, 0.3870194 , 0.43120503, 0.28535628, 0.61384559,\n",
              "        0.54589224, 0.337497  , 0.36438727, 0.32359576, 0.8205018 ]),\n",
              " 'score_time': array([0.00551701, 0.00441933, 0.00451493, 0.00442505, 0.0044682 ,\n",
              "        0.00447702, 0.0044806 , 0.00455165, 0.0043757 , 0.00432873]),\n",
              " 'test_accuracy': array([0.63636364, 0.63636364, 0.68831169, 0.74025974, 0.72727273,\n",
              "        0.76623377, 0.77922078, 0.66233766, 0.68421053, 0.73684211]),\n",
              " 'test_precision': array([0.48484848, 0.48387097, 0.54545455, 0.62962963, 0.75      ,\n",
              "        0.71428571, 0.72727273, 0.51162791, 0.75      , 0.75      ]),\n",
              " 'test_recall': array([0.59259259, 0.55555556, 0.66666667, 0.62962963, 0.33333333,\n",
              "        0.55555556, 0.59259259, 0.81481481, 0.11538462, 0.34615385])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2N7yzQRgh5V",
        "colab_type": "text"
      },
      "source": [
        "**cv=10 is provided, which means we are performing 10 fold cross validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxN-rYmigh5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4619d5ef-efce-4839-994c-d6bab154a7d3"
      },
      "source": [
        "print(\"Accuracy: \", cv_results[\"test_accuracy\"].mean())\n",
        "print(\"Precision: \", cv_results[\"test_precision\"].mean())\n",
        "print(\"Recall: \", cv_results[\"test_recall\"].mean())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7057416267942583\n",
            "Precision:  0.6346989976209783\n",
            "Recall:  0.5202279202279201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrNp0R93gh5a",
        "colab_type": "text"
      },
      "source": [
        "**For all valid scoring options - use the following:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mz4G3U5gh5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b187f006-a198-4df7-d843-d9673865f4cc"
      },
      "source": [
        "import sklearn.metrics as m\n",
        "m.SCORERS.keys()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygp5tnDdgh5q",
        "colab_type": "text"
      },
      "source": [
        "For more complicated scoring metrics (such as specificity, which isn't explicilty provided by sklearn), or to create your own metrics, \n",
        "http://scikit-learn.org/stable/modules/model_evaluation.html#using-multiple-metric-evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoVnDQsigh5x",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Leave One Out Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDxbxH6ggh5x",
        "colab_type": "text"
      },
      "source": [
        "<blockquote>This code takes a long time to run, you can either skip running this part and directly just see the printed results, or wait for 10-15 mins for this to run </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZEbpVaygh5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import LeaveOneOut"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vBvtai8Xgh55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9767ffa-2497-437a-d792-a4805658609b"
      },
      "source": [
        "cv_results = cross_validate(mlp, X, y,\n",
        "                            cv=LeaveOneOut(), scoring=[\"accuracy\"])\n",
        "cv_results"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': array([0.69438958, 0.60249639, 0.6092515 , 0.50150824, 0.37436223,\n",
              "        0.56539226, 0.90456223, 0.36669683, 0.6634295 , 0.34892321,\n",
              "        0.60973597, 0.38392138, 0.42026472, 0.37890434, 0.47635007,\n",
              "        0.35877681, 0.95338631, 0.20462275, 0.40883422, 0.69038463,\n",
              "        0.59254718, 0.51050305, 0.84989786, 0.63648367, 0.58165002,\n",
              "        0.52374244, 0.40113211, 0.58202124, 0.54756069, 0.16724753,\n",
              "        0.51132321, 0.67998934, 0.53630233, 0.97578168, 0.347013  ,\n",
              "        0.35797286, 0.36268973, 0.3718574 , 1.03278422, 0.4895463 ,\n",
              "        0.63920665, 0.81156778, 0.47146368, 0.49167371, 0.42675614,\n",
              "        0.58393407, 0.52894711, 0.6210444 , 0.21428728, 0.5754838 ,\n",
              "        0.32855439, 0.39590931, 0.38858128, 0.36249089, 0.37812161,\n",
              "        0.4428308 , 0.495296  , 0.86156154, 0.33912826, 0.53542471,\n",
              "        0.50740147, 0.699435  , 0.6056869 , 0.37705135, 0.59179235,\n",
              "        0.53193855, 1.12369585, 0.36612701, 0.4596839 , 0.21729994,\n",
              "        0.64767718, 0.53064847, 0.43887043, 0.67424178, 0.39696074,\n",
              "        0.54517817, 0.31858039, 0.33339691, 0.7187655 , 1.00468922,\n",
              "        0.30663633, 0.64213228, 0.59402227, 0.62729859, 0.89808345,\n",
              "        0.33779597, 0.460217  , 0.53890896, 0.25035548, 0.40739512,\n",
              "        0.57978606, 0.61218596, 0.38741136, 0.67041659, 1.10827947,\n",
              "        0.32685566, 0.65942144, 0.78871059, 0.5836091 , 0.19877267,\n",
              "        0.50973845, 0.41760254, 0.32306528, 0.41000462, 0.47993326,\n",
              "        0.76991725, 0.37040925, 0.42919445, 0.3858819 , 0.38323069,\n",
              "        0.55374289, 0.84512281, 0.50693822, 0.2983768 , 0.27148938,\n",
              "        0.74490714, 0.56679821, 0.20825577, 0.6463182 , 0.52529836,\n",
              "        0.63256383, 0.61839223, 0.21260834, 0.28259492, 0.36102247,\n",
              "        0.65538955, 0.5386467 , 0.78601789, 1.11863828, 0.52031112,\n",
              "        0.29139805, 0.4763515 , 0.33974242, 0.81417727, 0.54554915,\n",
              "        0.51699281, 0.48621106, 0.36483979, 0.55837417, 0.53437781,\n",
              "        0.69351411, 0.49875689, 0.37062287, 0.6125195 , 0.41944909,\n",
              "        0.48464561, 0.67859507, 0.38400531, 0.81253695, 0.43196464,\n",
              "        0.52286983, 0.36962342, 0.53018928, 0.32981658, 0.32357788,\n",
              "        0.37277102, 0.35235524, 0.58325553, 0.40753794, 0.43376613,\n",
              "        0.36263776, 0.45733261, 0.44781065, 0.57164907, 0.27890754,\n",
              "        0.57850933, 0.29750848, 0.45144486, 0.71252847, 0.33229113,\n",
              "        0.68377781, 0.47488785, 0.47879863, 0.57977891, 0.8006804 ,\n",
              "        0.73518801, 0.59189773, 0.55084896, 0.23865557, 0.71613955,\n",
              "        0.49028659, 0.57490683, 0.43347001, 0.61566782, 0.40412116,\n",
              "        0.73033547, 0.33846736, 0.64678025, 0.55168033, 0.73964214,\n",
              "        0.53279519, 0.63378191, 0.36809039, 0.42690444, 0.46132612,\n",
              "        0.52738428, 0.43883801, 0.2826159 , 0.48619318, 0.45825982,\n",
              "        0.79366565, 0.63427997, 0.41769218, 0.35372329, 0.53568292,\n",
              "        0.68242669, 0.23136806, 0.50688338, 0.57757425, 0.67323637,\n",
              "        0.49990416, 0.45111275, 0.52442813, 0.78564906, 0.50793457,\n",
              "        0.45020747, 0.3876965 , 0.3735671 , 0.3446126 , 0.2475419 ,\n",
              "        0.52676821, 0.73858261, 0.35028815, 0.46089435, 0.5638454 ,\n",
              "        0.22398829, 0.30219936, 0.64079714, 0.51293111, 0.36898565,\n",
              "        0.47228217, 0.32739735, 0.40957689, 0.26924348, 0.43213296,\n",
              "        0.4605875 , 0.41872168, 0.76636815, 0.38558578, 0.71554542,\n",
              "        0.62682033, 0.39381552, 0.40006495, 0.2694881 , 0.82610369,\n",
              "        0.3313427 , 0.53129077, 0.16880131, 0.4496541 , 0.50426459,\n",
              "        0.49533629, 0.57118726, 0.5187943 , 0.46561527, 0.29306936,\n",
              "        0.70746899, 0.55864167, 0.27944899, 0.59121919, 0.50560522,\n",
              "        0.53299904, 1.17827964, 0.50003123, 0.74165392, 0.33835959,\n",
              "        0.4181006 , 0.39553785, 0.2250483 , 0.83518267, 0.56375051,\n",
              "        0.63281727, 0.68008757, 0.73604178, 0.43820262, 0.58341789,\n",
              "        0.8701942 , 0.3865664 , 0.54166222, 0.36962175, 0.39734077,\n",
              "        0.49900293, 0.59701085, 1.29657936, 0.24192405, 0.31145978,\n",
              "        0.58091712, 0.32225966, 0.84422851, 0.64635086, 0.36524963,\n",
              "        0.55969381, 0.5857017 , 0.26662517, 1.15142417, 0.9322958 ,\n",
              "        0.47318029, 0.41764665, 0.49735308, 1.04765201, 0.27520108,\n",
              "        0.85513473, 0.32798529, 0.27996445, 0.62704682, 0.9545455 ,\n",
              "        0.26365709, 0.58806682, 0.80918121, 0.51512766, 0.52262354,\n",
              "        0.30785561, 0.52361512, 0.28917003, 0.49120355, 0.28455496,\n",
              "        0.36884117, 0.36824322, 0.49721885, 0.56907749, 0.77602863,\n",
              "        0.48534846, 0.71297646, 0.53513265, 0.70798349, 0.44001818,\n",
              "        0.78486466, 0.29285383, 0.62179255, 0.45768166, 0.24168539,\n",
              "        0.6141963 , 0.78716063, 0.46593595, 0.25644541, 0.44380569,\n",
              "        0.61674857, 0.51637387, 0.50701284, 0.6354413 , 0.5534668 ,\n",
              "        0.34529638, 0.62697911, 0.83402896, 0.25933194, 0.69683552,\n",
              "        0.47381806, 0.52511668, 0.78892994, 0.41919422, 0.45229697,\n",
              "        0.21392822, 0.37850714, 0.57680774, 0.35299253, 0.68106985,\n",
              "        0.57254243, 0.41433859, 0.49942017, 0.49291873, 0.35334563,\n",
              "        0.46461821, 1.05792165, 0.69015837, 0.24001336, 0.92236114,\n",
              "        0.71488285, 0.5463953 , 0.56725979, 0.64801621, 0.59090877,\n",
              "        0.43005371, 0.63814402, 0.50371575, 0.44003057, 0.41590047,\n",
              "        0.40473223, 0.56310797, 0.49834609, 0.24776077, 0.41077375,\n",
              "        0.3154192 , 0.31692123, 0.28765655, 0.55802822, 0.88872361,\n",
              "        0.62036705, 0.64531755, 0.18942261, 0.5531292 , 0.52067208,\n",
              "        0.60909081, 0.44658208, 0.93064785, 0.49678516, 0.37060261,\n",
              "        0.45013094, 0.36445284, 0.49386334, 0.70457077, 0.51637197,\n",
              "        0.3847003 , 0.46878409, 0.43959093, 0.87781739, 0.72485447,\n",
              "        0.2876761 , 0.55047774, 0.68466187, 0.72916579, 0.4653964 ,\n",
              "        0.30755472, 0.31145811, 0.63007927, 0.60380721, 0.67463064,\n",
              "        0.47599792, 0.72227001, 0.66134334, 0.21665692, 0.5321188 ,\n",
              "        0.36311054, 0.21341562, 0.64206862, 0.62171197, 0.59951115,\n",
              "        0.81522846, 0.57568431, 0.56282377, 0.54115272, 0.85063052,\n",
              "        0.3327651 , 0.68099594, 0.75308013, 0.56361818, 0.6507957 ,\n",
              "        0.77235365, 0.42061234, 0.48935914, 0.42541051, 0.53439307,\n",
              "        0.44911623, 0.65703797, 0.64188933, 0.32926965, 0.81252456,\n",
              "        0.95320296, 0.48566556, 0.64800429, 0.28693342, 0.46296573,\n",
              "        0.55293727, 0.49911237, 0.99440169, 0.91069555, 0.72801805,\n",
              "        1.03002381, 0.6755507 , 0.78124094, 0.35377693, 0.56122231,\n",
              "        0.33164358, 0.46501589, 0.30407906, 0.36170721, 0.68850636,\n",
              "        0.54924893, 0.59751439, 0.59675241, 0.82640266, 0.76362038,\n",
              "        0.37896204, 0.53593183, 0.47437882, 0.37266469, 0.54495907,\n",
              "        0.36991715, 0.56230235, 0.5913558 , 0.33473921, 0.8350966 ,\n",
              "        0.28476191, 0.46783209, 0.50229096, 0.5624876 , 0.86725497,\n",
              "        0.5345633 , 0.43050146, 0.31177044, 0.37759924, 0.3230505 ,\n",
              "        0.86278272, 0.72174168, 0.37870026, 0.83829236, 0.32222009,\n",
              "        0.54698563, 0.37353754, 0.68487191, 0.53336358, 0.42070508,\n",
              "        0.33977771, 0.77282286, 0.39967847, 0.40990448, 0.54410625,\n",
              "        0.95360065, 0.70930552, 0.55978584, 0.86554766, 0.80174947,\n",
              "        0.32051206, 0.44437242, 0.27009821, 0.68257475, 0.54372978,\n",
              "        0.42943454, 0.67337394, 0.88528752, 0.30893779, 0.51008558,\n",
              "        0.64556718, 0.58768296, 0.38027763, 0.63568234, 0.62618423,\n",
              "        0.37374258, 0.76166677, 0.5503633 , 0.46891952, 0.53930831,\n",
              "        0.33274961, 0.25592709, 0.6156528 , 0.5212276 , 0.21159148,\n",
              "        0.53348994, 0.51350331, 0.49475622, 0.37064171, 0.4782927 ,\n",
              "        0.80111241, 0.68477345, 0.62783766, 0.56636262, 0.1906054 ,\n",
              "        0.32526255, 0.49415183, 1.26113558, 0.81007457, 0.57269144,\n",
              "        0.46271896, 0.44202161, 0.41653299, 0.52039528, 0.81643295,\n",
              "        0.40415716, 0.43534923, 0.3332262 , 0.65964127, 0.55678844,\n",
              "        0.60032797, 0.32144833, 0.50524187, 0.27830625, 0.06387615,\n",
              "        0.48823357, 0.54948807, 0.48499942, 0.60254788, 0.4802022 ,\n",
              "        0.7646451 , 0.41053581, 0.6012485 , 0.31416273, 0.44050384,\n",
              "        0.48672986, 0.52806282, 0.71973014, 0.55816936, 0.61991739,\n",
              "        0.35454583, 0.6636548 , 0.69073892, 0.40340829, 0.37283301,\n",
              "        0.49433231, 0.34263611, 0.43493462, 0.46061826, 0.80976987,\n",
              "        0.48903608, 0.34574556, 0.51501679, 0.75032115, 0.47219491,\n",
              "        0.55686641, 0.5803256 , 0.33565807, 0.75013614, 0.21677709,\n",
              "        0.73820376, 0.55042362, 0.52118874, 0.6015017 , 0.64493752,\n",
              "        0.92525125, 0.84240055, 0.76138568, 0.6290369 , 0.78474855,\n",
              "        0.78087449, 0.36527038, 0.3977983 , 0.71901798, 0.96290135,\n",
              "        0.50053883, 0.54802203, 0.36690521, 0.39416337, 0.40580487,\n",
              "        0.55641222, 0.72092581, 0.34367418, 0.87125587, 0.53123045,\n",
              "        0.45943284, 0.85640454, 0.45896387, 0.48403597, 0.49020433,\n",
              "        0.50818658, 0.36403894, 0.50927591, 0.3804996 , 0.42820024,\n",
              "        0.55565   , 0.54210091, 0.33158875, 0.59942341, 0.58492327,\n",
              "        0.67719865, 0.40386605, 0.34494972, 0.54054213, 0.4298389 ,\n",
              "        0.46173191, 0.50146675, 0.51997852, 0.50102448, 0.46013665,\n",
              "        0.33713198, 0.40220284, 0.68104339, 0.23586249, 0.60342979,\n",
              "        0.42878222, 0.60931849, 0.84085655, 0.38703966, 0.42613554,\n",
              "        0.34912205, 0.36162615, 0.79256272, 0.5147934 , 0.56666493,\n",
              "        0.74906063, 0.45751429, 0.43418384, 0.38033962, 0.481848  ,\n",
              "        0.3886795 , 0.32198405, 0.23294139, 0.15751624, 0.39099431,\n",
              "        0.45093489, 0.33772659, 0.42583489, 0.47729754, 0.41308618,\n",
              "        0.33312154, 0.45197058, 0.35371208, 0.42396307, 0.45313358,\n",
              "        0.37189865, 0.63841033, 0.3945365 , 0.63911915, 0.52937102,\n",
              "        0.70517635, 0.4279058 , 0.50223041, 0.580832  , 0.61076498,\n",
              "        0.29756427, 0.30213571, 0.54436207, 0.62513804, 0.68611121,\n",
              "        0.81523061, 0.61843014, 0.43877029, 0.59668517, 0.19834352,\n",
              "        0.47238517, 0.94923639, 0.52802324, 0.6357286 , 0.23466158,\n",
              "        0.51438451, 0.65070701, 0.47409081, 0.84499955, 0.47515631,\n",
              "        0.99588704, 0.64414597, 0.60976434, 0.79007435, 0.52629066,\n",
              "        0.85372114, 0.57431054, 0.60678864, 0.4605453 , 0.65919304,\n",
              "        0.41515183, 0.67391944, 0.27668929, 0.52622271, 0.38103342,\n",
              "        1.12914634, 0.22163177, 0.38083744, 0.50696063, 0.59720755,\n",
              "        0.57328796, 0.59471273, 0.30003285, 1.17000604, 0.45432377,\n",
              "        0.30096555, 0.78647518, 0.695122  , 0.66429973, 0.44947624,\n",
              "        0.70965624, 0.57826996, 0.49708533, 0.41490555, 0.58390379,\n",
              "        0.3922987 , 0.70164657, 0.54845142, 0.28652096, 0.5642364 ,\n",
              "        0.58787751, 0.75727463, 0.44896722, 0.55288005, 0.34214759,\n",
              "        0.41635108, 0.60868692, 0.81563878, 0.63989687, 0.47925186,\n",
              "        0.65784574, 0.30820107, 0.57961249]),\n",
              " 'score_time': array([0.0018611 , 0.00183916, 0.00197005, 0.00181937, 0.00187325,\n",
              "        0.0018723 , 0.00198293, 0.00185776, 0.00188351, 0.00241208,\n",
              "        0.00198126, 0.00189805, 0.00186157, 0.0018487 , 0.00185704,\n",
              "        0.00183082, 0.00187612, 0.00183034, 0.00185585, 0.0018425 ,\n",
              "        0.0018301 , 0.00180626, 0.0018549 , 0.00202227, 0.00187802,\n",
              "        0.00186419, 0.00182652, 0.0020051 , 0.00179601, 0.0018611 ,\n",
              "        0.00182414, 0.00182104, 0.00205827, 0.0020144 , 0.00182104,\n",
              "        0.00183272, 0.00180364, 0.00187588, 0.00184894, 0.00183535,\n",
              "        0.0019331 , 0.00188851, 0.00192785, 0.00186157, 0.00184631,\n",
              "        0.00184917, 0.00181437, 0.00184345, 0.0018332 , 0.00187421,\n",
              "        0.00184298, 0.00179625, 0.00186038, 0.00185823, 0.00184679,\n",
              "        0.00190043, 0.00187039, 0.00203943, 0.00206614, 0.00192785,\n",
              "        0.00191832, 0.00186419, 0.001863  , 0.00184989, 0.00195837,\n",
              "        0.00192547, 0.00181079, 0.0018208 , 0.00187922, 0.00205231,\n",
              "        0.00196433, 0.00175071, 0.00200272, 0.00183034, 0.00176978,\n",
              "        0.00181723, 0.00188494, 0.001858  , 0.00188994, 0.00198054,\n",
              "        0.00184059, 0.00194883, 0.00183797, 0.00186658, 0.0018003 ,\n",
              "        0.00186682, 0.00190568, 0.00199366, 0.00185871, 0.00186014,\n",
              "        0.00189066, 0.00182295, 0.00183892, 0.00184822, 0.00185704,\n",
              "        0.00171971, 0.0017457 , 0.0019002 , 0.00186253, 0.00185513,\n",
              "        0.00185513, 0.00190639, 0.00191593, 0.00186181, 0.00190639,\n",
              "        0.0018611 , 0.00190067, 0.00191307, 0.00191665, 0.00182128,\n",
              "        0.00179791, 0.00188541, 0.00183344, 0.00183296, 0.0018177 ,\n",
              "        0.00187111, 0.00248408, 0.00192928, 0.00192785, 0.00184894,\n",
              "        0.0018158 , 0.002074  , 0.00193238, 0.00182724, 0.0018394 ,\n",
              "        0.00187159, 0.00193071, 0.0018115 , 0.00197411, 0.00188065,\n",
              "        0.00174713, 0.0018208 , 0.00181127, 0.00182509, 0.00178123,\n",
              "        0.00185204, 0.0019033 , 0.0018692 , 0.00196886, 0.00182486,\n",
              "        0.00189805, 0.00195432, 0.00179839, 0.00183725, 0.00179029,\n",
              "        0.00183988, 0.00182152, 0.0018363 , 0.00199223, 0.00185966,\n",
              "        0.00184751, 0.00181556, 0.00187135, 0.00186086, 0.00183392,\n",
              "        0.00182843, 0.00178027, 0.00183678, 0.00180125, 0.00184011,\n",
              "        0.00183034, 0.00179553, 0.00178242, 0.0018084 , 0.0017581 ,\n",
              "        0.00181913, 0.00180984, 0.00182295, 0.0018096 , 0.00186729,\n",
              "        0.00181842, 0.00178456, 0.00178456, 0.0018363 , 0.00181127,\n",
              "        0.00183797, 0.00178862, 0.00180864, 0.0018177 , 0.00188208,\n",
              "        0.00183606, 0.00183606, 0.00186086, 0.00170088, 0.00186157,\n",
              "        0.00184917, 0.00186801, 0.00185251, 0.00175786, 0.00181723,\n",
              "        0.00178742, 0.00210452, 0.00184417, 0.00180006, 0.00178552,\n",
              "        0.00178528, 0.00180483, 0.0017736 , 0.00183439, 0.00175738,\n",
              "        0.00184345, 0.00178933, 0.00186443, 0.0018425 , 0.00183201,\n",
              "        0.00186086, 0.00184464, 0.00174832, 0.00222421, 0.00185132,\n",
              "        0.00195599, 0.00183821, 0.00174141, 0.00198984, 0.00179982,\n",
              "        0.0018661 , 0.00181389, 0.00190806, 0.0018878 , 0.00185609,\n",
              "        0.00183034, 0.00187492, 0.00184274, 0.00178432, 0.00182724,\n",
              "        0.00179338, 0.00177479, 0.00193   , 0.00179076, 0.00186896,\n",
              "        0.00178719, 0.00185609, 0.00179982, 0.00180936, 0.00184155,\n",
              "        0.0018425 , 0.00179958, 0.0019145 , 0.00175548, 0.00185657,\n",
              "        0.00181127, 0.00179744, 0.00183272, 0.00181007, 0.00178599,\n",
              "        0.00180626, 0.00184178, 0.00177002, 0.00214338, 0.00180221,\n",
              "        0.00184321, 0.0017786 , 0.00184298, 0.00179982, 0.00180674,\n",
              "        0.00183487, 0.00186777, 0.00180674, 0.00184917, 0.00181198,\n",
              "        0.00187016, 0.00185156, 0.00179863, 0.00181341, 0.00178838,\n",
              "        0.00175071, 0.00180507, 0.00179768, 0.00184536, 0.00180864,\n",
              "        0.00181293, 0.00176454, 0.00180197, 0.00181603, 0.00179672,\n",
              "        0.00181794, 0.00182939, 0.00185442, 0.00179315, 0.00177193,\n",
              "        0.00180387, 0.00183272, 0.00183105, 0.00178647, 0.00185108,\n",
              "        0.00180435, 0.00186181, 0.00197268, 0.0017786 , 0.00180745,\n",
              "        0.00178337, 0.00181317, 0.00176978, 0.00181699, 0.00177836,\n",
              "        0.00185823, 0.00189853, 0.00179243, 0.00180769, 0.00180793,\n",
              "        0.00179076, 0.00179362, 0.00331616, 0.00180221, 0.00180078,\n",
              "        0.0017724 , 0.00180626, 0.00180292, 0.00177121, 0.00180864,\n",
              "        0.00178456, 0.00177693, 0.00178194, 0.00187492, 0.0017848 ,\n",
              "        0.00204754, 0.00182104, 0.00183535, 0.00182486, 0.00360346,\n",
              "        0.00187135, 0.00177479, 0.00180149, 0.00195742, 0.00176072,\n",
              "        0.00180173, 0.00178289, 0.00179172, 0.00196052, 0.0018301 ,\n",
              "        0.00186682, 0.00177979, 0.00184464, 0.00187016, 0.00180292,\n",
              "        0.00182223, 0.00179243, 0.00178671, 0.0018487 , 0.00180674,\n",
              "        0.001894  , 0.00184035, 0.00186324, 0.00183988, 0.00184917,\n",
              "        0.00177693, 0.00183344, 0.00197506, 0.00187087, 0.00195336,\n",
              "        0.00178909, 0.00189829, 0.00176811, 0.00191379, 0.00181723,\n",
              "        0.00183654, 0.00181866, 0.00178742, 0.00181675, 0.00186324,\n",
              "        0.00184512, 0.00183797, 0.00183582, 0.00179863, 0.0018177 ,\n",
              "        0.00185704, 0.00191307, 0.00182724, 0.00179911, 0.00183868,\n",
              "        0.00180936, 0.00170231, 0.00196648, 0.00180483, 0.00182581,\n",
              "        0.00180745, 0.00178432, 0.00182509, 0.0019536 , 0.00183535,\n",
              "        0.00181055, 0.00184846, 0.0018239 , 0.00179887, 0.00181603,\n",
              "        0.0018549 , 0.00184727, 0.00185323, 0.00183702, 0.00184727,\n",
              "        0.00179887, 0.00186181, 0.00174451, 0.00180054, 0.00185871,\n",
              "        0.00180101, 0.00178957, 0.00181913, 0.00181127, 0.0018127 ,\n",
              "        0.00181341, 0.00180292, 0.00183344, 0.0017767 , 0.00183749,\n",
              "        0.00820446, 0.00182199, 0.00180507, 0.00202131, 0.00180435,\n",
              "        0.00196195, 0.00180364, 0.0019629 , 0.00184178, 0.00181818,\n",
              "        0.00181508, 0.00177526, 0.00176144, 0.00177002, 0.00180435,\n",
              "        0.00180221, 0.00179195, 0.00182867, 0.00203943, 0.00187659,\n",
              "        0.00186658, 0.00186992, 0.00177884, 0.00186181, 0.00179148,\n",
              "        0.00191236, 0.00186229, 0.00178647, 0.00178695, 0.00182271,\n",
              "        0.00179791, 0.00180817, 0.0017817 , 0.00181627, 0.0018189 ,\n",
              "        0.00184417, 0.0018158 , 0.00183225, 0.00182486, 0.00173521,\n",
              "        0.00179124, 0.00184059, 0.00196481, 0.00188518, 0.0018394 ,\n",
              "        0.00188994, 0.00184321, 0.00185609, 0.00201273, 0.00179005,\n",
              "        0.00189424, 0.00184631, 0.00188184, 0.00188327, 0.001827  ,\n",
              "        0.00182009, 0.00182843, 0.0018146 , 0.00179768, 0.00185132,\n",
              "        0.00184345, 0.00173879, 0.0018115 , 0.00183415, 0.00183177,\n",
              "        0.00177479, 0.00182796, 0.00179362, 0.00182319, 0.00181341,\n",
              "        0.00178933, 0.00177908, 0.00178862, 0.00184226, 0.00181699,\n",
              "        0.00184631, 0.00184822, 0.00183439, 0.00183272, 0.00186419,\n",
              "        0.00183582, 0.00183177, 0.00204825, 0.001899  , 0.00181794,\n",
              "        0.00182223, 0.00185823, 0.00184393, 0.00183153, 0.00178075,\n",
              "        0.00181031, 0.00182605, 0.00184965, 0.00187278, 0.00191212,\n",
              "        0.00177741, 0.00193644, 0.00184727, 0.00182486, 0.0018208 ,\n",
              "        0.0018239 , 0.0018096 , 0.00185227, 0.001863  , 0.00193834,\n",
              "        0.00187707, 0.00186706, 0.00183415, 0.00191808, 0.00189877,\n",
              "        0.00184584, 0.001858  , 0.00186086, 0.00175786, 0.00231385,\n",
              "        0.00194716, 0.00182796, 0.00179458, 0.00185466, 0.00183177,\n",
              "        0.0019207 , 0.00187778, 0.00194359, 0.00188255, 0.00183249,\n",
              "        0.00189614, 0.00179863, 0.00176477, 0.00202274, 0.00197792,\n",
              "        0.00187635, 0.00185609, 0.00180435, 0.00189376, 0.00181794,\n",
              "        0.00183034, 0.0018661 , 0.00182891, 0.00179195, 0.00185084,\n",
              "        0.00189066, 0.00178075, 0.00184155, 0.00179863, 0.00179195,\n",
              "        0.00198078, 0.0018034 , 0.00183105, 0.00190449, 0.00191069,\n",
              "        0.00185418, 0.0018816 , 0.00203848, 0.00184369, 0.00189567,\n",
              "        0.0019238 , 0.00182366, 0.00191665, 0.00184178, 0.00174284,\n",
              "        0.00182128, 0.00220656, 0.00182056, 0.00195312, 0.00180888,\n",
              "        0.00239491, 0.00180674, 0.00193667, 0.00181079, 0.00182962,\n",
              "        0.00196314, 0.00183702, 0.0018599 , 0.0017941 , 0.0020237 ,\n",
              "        0.00183654, 0.00181866, 0.00181222, 0.00188613, 0.00195074,\n",
              "        0.00182629, 0.00181317, 0.00181103, 0.00182939, 0.0018692 ,\n",
              "        0.00192475, 0.00188279, 0.00178909, 0.00187063, 0.00178123,\n",
              "        0.00185609, 0.00187564, 0.00184774, 0.00192809, 0.0017786 ,\n",
              "        0.00182796, 0.00181937, 0.00187206, 0.00184631, 0.00187302,\n",
              "        0.00185943, 0.00184464, 0.0018754 , 0.00181603, 0.00183344,\n",
              "        0.0019381 , 0.00184631, 0.00184011, 0.00183797, 0.00191689,\n",
              "        0.00188041, 0.00176668, 0.00183606, 0.00182867, 0.00197816,\n",
              "        0.00200582, 0.00181293, 0.00186706, 0.00182223, 0.0018816 ,\n",
              "        0.00179219, 0.00197601, 0.00180745, 0.0018959 , 0.0018599 ,\n",
              "        0.0019052 , 0.00184321, 0.00182438, 0.00177407, 0.00179768,\n",
              "        0.00180125, 0.00182247, 0.00180197, 0.0017426 , 0.00180578,\n",
              "        0.00192451, 0.00180101, 0.00180674, 0.00198221, 0.00179124,\n",
              "        0.00181365, 0.00179529, 0.00182438, 0.00178027, 0.0018723 ,\n",
              "        0.00178003, 0.0018115 , 0.00179172, 0.00189376, 0.00176716,\n",
              "        0.00186491, 0.00181675, 0.00184584, 0.00178289, 0.00180101,\n",
              "        0.00175047, 0.00185609, 0.00185537, 0.00189805, 0.00178838,\n",
              "        0.00182104, 0.0018363 , 0.0017662 , 0.00180697, 0.00182486,\n",
              "        0.00175762, 0.00191498, 0.00179338, 0.00182676, 0.00184917,\n",
              "        0.00180602, 0.00181293, 0.00186706, 0.00187993, 0.00183439,\n",
              "        0.00198865, 0.00195146, 0.00203967, 0.00195241, 0.00180721,\n",
              "        0.00185561, 0.00211668, 0.00185108, 0.00193453, 0.00177503,\n",
              "        0.00190473, 0.00181651, 0.00174499, 0.00180817, 0.00183558,\n",
              "        0.00176215, 0.00187159, 0.00182128, 0.00179553, 0.00189281,\n",
              "        0.00189877, 0.00186753, 0.00185394, 0.00203228, 0.00184584,\n",
              "        0.00195336, 0.00198793, 0.00186372, 0.0019424 , 0.00178552,\n",
              "        0.00204611, 0.00199008, 0.002033  , 0.00189376, 0.00337648,\n",
              "        0.00179052, 0.00200653, 0.00187516, 0.00187278, 0.00190306,\n",
              "        0.0019269 , 0.00190353, 0.00178289, 0.00184274, 0.00182629,\n",
              "        0.00181484, 0.00179458, 0.00184345, 0.00182915, 0.00183535,\n",
              "        0.00185037, 0.00180268, 0.00178123, 0.00178766, 0.0018177 ,\n",
              "        0.00182223, 0.00182605, 0.00317574, 0.00200438, 0.00183725,\n",
              "        0.00181127, 0.00181818, 0.00181699, 0.00183845, 0.00178409,\n",
              "        0.00183082, 0.0018456 , 0.00185251, 0.00187731, 0.00185585,\n",
              "        0.00173259, 0.00198364, 0.00183201, 0.00185347, 0.00183225,\n",
              "        0.00189853, 0.00176835, 0.00198627, 0.00187826, 0.00183082,\n",
              "        0.00179887, 0.0018599 , 0.00184226, 0.00182199, 0.00178933,\n",
              "        0.00183821, 0.00182462, 0.00181603]),\n",
              " 'test_accuracy': array([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
              "        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
              "        0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
              "        1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
              "        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
              "        1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
              "        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
              "        0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
              "        0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
              "        1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
              "        0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
              "        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
              "        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
              "        1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
              "        0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
              "        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
              "        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
              "        1., 1., 1.])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VXdvPbqgh6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9be05683-e28f-41f0-943a-510859b648ed"
      },
      "source": [
        "cv_results['test_accuracy'].mean()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6861979166666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh7CwmWygh6b",
        "colab_type": "text"
      },
      "source": [
        "We have not included precision and recall in the metrics here. Can you think why?  \n",
        "**<mark>Hint:</mark> Imagine the confusion matrix when the testing has only one sample**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1GyHfC0gh6j",
        "colab_type": "text"
      },
      "source": [
        "## 3. Hyperparameter Tuning\n",
        "Hyperparameters are important parts of the ML model and can make the model gold or trash. Here we have discussed one of the popular hyperparameter tunning method i.e. using Grid Search CV. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEHcCWcegh6l",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Grid Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQwxdMUkgh6m",
        "colab_type": "text"
      },
      "source": [
        "### 3.1.1 Crime Rate- Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qVEuJimgh6n",
        "colab_type": "text"
      },
      "source": [
        "**Predictor Variable: Crime Rate (Regression Based)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wbh15ZTgh6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NamrnJjOgh6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crime = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/Standard%20Metropolitan%20Areas%20Data%20-%20train_data.csv\")\n",
        "train, test = train_test_split(crime)\n",
        "x_train = train.drop('crime_rate', axis = 1)     \n",
        "y_train = train.crime_rate      \n",
        "x_test = test.drop('crime_rate', axis = 1)      \n",
        "y_test = test.crime_rate        "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DZvhZ0fgh66",
        "colab_type": "text"
      },
      "source": [
        "Performance without grid search: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TjevVRegh67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2rzRQoBgh7D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ae5770b-270e-4202-d6df-39bd4090f8b0"
      },
      "source": [
        "mean_squared_error(y_test, y_pred, squared=False)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.726219455300324"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no8Vw1Qmgh7N",
        "colab_type": "text"
      },
      "source": [
        "Performance with Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UPswE7Ogh7O",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Define a parameter Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6084pRAvgh7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False], 'n_jobs':[-1,1,10,15]}"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPQl8cdHgh7Y",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Fit the model to find the best hyperparameters on training data, and select the scorer you want to select to optimise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSWCh9rDgh7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0f7459fc-c75b-4bf9-c758-3a960364657d"
      },
      "source": [
        "grid = GridSearchCV(lr,parameters, cv=3)\n",
        "grid.fit(x_train, y_train)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=LinearRegression(copy_X=True, fit_intercept=True,\n",
              "                                        n_jobs=None, normalize=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'copy_X': [True, False],\n",
              "                         'fit_intercept': [True, False],\n",
              "                         'n_jobs': [-1, 1, 10, 15],\n",
              "                         'normalize': [True, False]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTdZBb40gh7d",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Print the best obtained parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIonXbIogh7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d146a496-2c42-4f80-cdc5-5542e9355077"
      },
      "source": [
        "grid.best_estimator_"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=-1, normalize=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Urv2jtngh7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe1ac2a6-1c08-410f-b2e6-daac32142087"
      },
      "source": [
        "grid_lr = LinearRegression(copy_X=True, fit_intercept=False, n_jobs=-1, normalize=False)\n",
        "grid_lr.fit(x_train, y_train)\n",
        "y_pred= grid_lr.predict(x_test)\n",
        "mean_squared_error(y_test, y_pred, squared=False)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4381465958909706"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwhEjnmGgh7v",
        "colab_type": "text"
      },
      "source": [
        "**Performance does not vary that much!**\n",
        "\n",
        "The number of hyperparameters for Linear Regression is very less. Hence all of them give similar performance (in this specific dataset)\n",
        "\n",
        "Let us try another parameter for which the performance varies a lot!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gp-Po0wgh7w",
        "colab_type": "text"
      },
      "source": [
        "### 3.1.2 Artificial Neural Network\n",
        "In Linear Regression, there are not many parameters to optimise, hence performance may not vary that much. In many other classifiers, there are a number of hyper parameters to tune, so let us see an example of how performance is improved using Grid Search. We take an example of **Artificial Neural Networks.**\n",
        "\n",
        "You need not understand the working behind ANN, so it is okay if you do not understand the parameter grid in detail. Let's just see how the performance improves by applying Grid Search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K28nx24bNTMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use diabetes data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2W1Ei3Ugh7y",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Define a parameter Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX_8tDiSgh7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcl_8hc6gh79",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Fit the model to find the best hyperparameters on training data, and select the scorer you want to select to optimise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc9VN84Ygh7-",
        "colab_type": "text"
      },
      "source": [
        "<blockquote> <i>  This code takes a long time to run, you can either skip running this part and directly just see the printed results, or wait for 10-15 mins for this to run </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBeiT6D1gh7_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "9176425f-aa5d-4eca-bf16-d0acb35afde1"
      },
      "source": [
        "mlp_random = GridSearchCV(mlp, parameter_space, scoring = 'accuracy')\n",
        "mlp_random.fit(x_train, y_train)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
              "                                     batch_size='auto', beta_1=0.9,\n",
              "                                     beta_2=0.999, early_stopping=False,\n",
              "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
              "                                     learning_rate='constant',\n",
              "                                     learning_rate_init=0.001, max_fun=15000,\n",
              "                                     max_iter=1000, momentum=0.9,\n",
              "                                     n_iter_no_change=10,\n",
              "                                     nesterovs_momentum=True, power_t=0.5,\n",
              "                                     random_s...\n",
              "                                     validation_fraction=0.1, verbose=False,\n",
              "                                     warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'activation': ['tanh', 'relu'],\n",
              "                         'alpha': [0.0001, 0.05],\n",
              "                         'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50),\n",
              "                                                (100,)],\n",
              "                         'learning_rate': ['constant', 'adaptive'],\n",
              "                         'solver': ['sgd', 'adam']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='accuracy', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8f-duE9gh8J",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Print the best obtained parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4bj3qClgh8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "420d1364-57d3-4c74-e1ad-6c2e67759df5"
      },
      "source": [
        "mlp_random.best_params_"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'tanh',\n",
              " 'alpha': 0.05,\n",
              " 'hidden_layer_sizes': (100,),\n",
              " 'learning_rate': 'adaptive',\n",
              " 'solver': 'adam'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmHmS7Nfgh8S",
        "colab_type": "text"
      },
      "source": [
        "**Step 4:** Train your model on these parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2repqpLogh8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_grid = MLPClassifier(solver='adam', learning_rate='constant', hidden_layer_sizes=(100,), alpha=0.05, \n",
        "                         activation='tanh',max_iter=2000)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCXtKB2Ugh8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_grid.fit(x_train, y_train)\n",
        "y_pred = mlp_grid.predict(x_test)\n",
        "acc_tuned = accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZTgmXQogh8e",
        "colab_type": "text"
      },
      "source": [
        "**Comparing with Accuracy from model without hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "H-kojEW-gh8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "639c4c3d-efc7-4fb6-851d-57e42b4568fa"
      },
      "source": [
        "print(\"Accuracy of Tuned model: \",np.round(acc_tuned,3))\n",
        "print(\"Accuracy of non-Tuned model: \",np.round(acc,3))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Tuned model:  0.708\n",
            "Accuracy of non-Tuned model:  0.688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsGwxzNOgh8h",
        "colab_type": "text"
      },
      "source": [
        "Approximately 5% difference in accuracy!  \n",
        "By including an even more exhaustive grid search, we can improve the performance even further"
      ]
    }
  ]
}